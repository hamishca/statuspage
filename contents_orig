{"active":"","inactive":"#### 08/29/2023 14:51hrs PT - 08/29/2023 15:06hrs PT\r\n\r\nWe experienced a degraded service for /v1/get_id_lists requests globally lasting around 15 minutes. 20% of requests were returned a 500 HTTP response code. During the time, segment updates on server SDKs may be delayed if changes were made at the time. However, SDKs have logic to retry pulling the updated segment list locally. The service is fully recovered now.\r\n\r\n#### 08/17/2023 23:54hrs PT - 08/18/2023 00:32hrs PT\r\n\r\nWe experienced a transient degraded service for logging events in US east region. During the time, SDKs in the region may experience elevated 5XX rates when sending events to our service. However, SDKs have logic to retry sending events to our service at a later time. The service is fully recovered now.\r\n\r\n#### 06/30/2023 12:00hrs PT - 06/30/2023 12:10hrs PT\r\n\r\nWe experienced technical difficulties from approximately 12:00 PM PST to 12:10 PM PST. During this time period, we had elevated 5xx rates for our client initialization endpoint which may have resulted in temporary delayed evaluation propagation.\r\n\r\nEvaluations auto-corrected from 12:05 PST -> 12:10 PST, and no user action is required.\r\n\r\n#### 06/15/2023 12:00hrs PT - 06/15/2023 17:00hrs PT\r\n\r\nUpdate 17:20 - Incident is resolved. If you are still seeing longer latency, please contact us on Slack.\r\n\r\nWe are experiencing degraded performance in our US east cluster. Some customers within US east region may experience elevated latency.\r\n\r\nWe are moving some API traffic away from US east. No action needed from users.\r\n\r\n#### 05/15/2023 10:31hrs PT - 05/17/2023 10:26hrs PT\r\n\r\nA performance optimization we made on May 15 at 10:31 AM PT resulted in our servers occasionally returning stale config specs. The end impact is that some gates, dynamic configs, experiments, etc. may have evaluated different results across different SDK initializations. Projects that frequently make changes on console (at least once every ~2min) would have likely never experienced this. The likelihood of being impacted increases as the frequency of changes decreases.\r\n\r\n#### 05/03/2023 9:19hrs PT - 05/03/2023 10:40hrs PT\r\nIncident impacting our API servers increasing 5XX rates across our API endpoints\r\n\r\n#### 05/01/2023 11:50hrs PT - 05/01/2023 15:50hrs PT\r\nWe just identified and fixed a bug impacting our log event API endpoint today. The bug caused us to lose tracking on experiment exposures during the window (11:50am PST - 3:50pm PST). Most experiments have users repeatedly exposed throughout the experiment's life, and will still be tracked in experiments they participate in. For experiments where users are expected to only see once, the impact is scoped to a small reduction in the estimation of experimental effects (worst case of 1% impact on a 2-week long experiment). Feature gating and experiment bucketing/assignment were not impacted. There is no impact on validity of experiments.\r\n\r\nTimeline\r\n\r\n- 11:50am PST - Bug hits production\r\n- 15:05pm PST - Anomaly alerts fired, investigation starts\r\n- 15:15pm PST - User reports noticed in Slack \r\n- 15:50pm PST - Issue resolved \r\n\r\nWe will post mortem the incident itself and pursue shortening the Mean Time to Detect/Discover on this class of issues.\r\n\r\n#### 04/23/2023 21:30hrs PT - 04/24/2023 07:50hrs PT\r\nIncident impacting our API servers from April 23 9:30pm PT to April 24 7:50am PT resulted in some changes made through the Statsig Console or Console API to propagate slowly to our API servers. Some changes to Gate targeting, rollout, or Experiments were not acknowledged until 7:50am PT. After the incident was fixed, all previous changes were propagated correctly.\r\n\r\n#### 04/12/2023 11:00hrs PT - 23:00hrs PT\r\nDaily computation of exposures and experimental \"pulse\" results were severely delayed.  No data was loss, but updated results for the latest day didn't land till late in the evening.\r\n\r\nThe root cause of this issue was throttling from our cloud services provider possibly due to a misconfigured spark job.  This has been fully resolved and data for all companies has landed or will land shortly.\r\n\r\n#### 04/12/2023 05:00hrs PT - 05:20hrs PT\r\nIncident impacting our API servers from around 5:00pm PT to 5:20pm PT. The impact of the incident was that around 50% of log event API calls during this period failed. Because of retries from the SDKs our estimate is that 30% of events sent during the period of time were dropped and are unrecoverable. \r\n\r\nDuring this incident, there were elevated errors in API calls used for initialization for client and server SDKs as well, however those errors seem to have persisted for a short period of time, from around 5:03pm PT to 5:06pm PT.\r\n\r\nThe root cause of the issue was a mistake resulting in misconfiguration of our DNS routing policies that caused all of our API traffic to go to one region instead of being distributed globally. API traffic routing only to that single region caused the elevated errors as the servers in that region were not able to handle the increase in traffic until it was able to scale up. \r\n\r\nThis has been fully resolved now and we apologize for the disruption of the service. We will conduct a thorough analysis on this incident to prevent similar mistakes from happening again in the future.\r\n\r\n#### 04/03/2023 09:32hrs PT - 04/03/2023 12:17hrs PT\r\n\r\nConsole change propagation delay affecting US West. Any changes made to a Statsig project since 9:32am PT today are not making it downstream to SDKs/download_config_specs.\r\n\r\n04/03/2023 12:17hrs PT - Issue resolved\r\n\r\n\r\n#### 12/06/2022 10:00hrs PT - 12/20/2022 23:00hrs PT\r\nAn issue that has been fixed affected our API servers from Dec 6th to Dec 20th resulted in approximately 10% of events sent to our API servers to be dropped. Specifically this issue affected servers in our US East region. Due to this issue, we may have lost tracking on some users for a particular session, but since most users have repeated exposures (sometimes in the same session) we believe this impact is minimal. Feature gating and experiment bucketing/assignment were not impacted.\r\n\r\nThe only impact from your side would be a small reduction in the estimation of experimental effects. In the worst case, results for a 95% confidence interval test would have increased confidence intervals equivalent to ~98%, though for most metrics, the loss of sensitivity is smaller. Any experiment decision made based off stat-sig metrics are still correct; only those made off borderline neutral results could have been affected.\r\n\r\n#### 11/28/2022 16:00hrs PT\r\n\r\nStatsig API is experiencing degradation in US-East-2 primarily, which could also be affecting other regions.  We have identified the issue and are working on a fix.\r\n\r\n11/28 17:00hrs PT - Issue has been mitigated\r\n11/28 17:10hrs PT - Fully Recovered\r\n\r\n#### 8/4/2022 11:00hrs PT\r\n\r\n08/06 00:00 PT - Console Data Delay - Console data for 8/3 and 8/4 is update-to-date. No delays are expected for 8/5 data. Please reach out to us through Slack if you are still experiencing issues. We apologize for the inconvenience it has brought and thank you for your patience. We will conduct a detailed analysis of the incident to prevent a future recurrence.\r\n\r\n08/05 15:30 PT - Console Data Delay - 8/3 Console data is up-to-date. Backfilling for 8/4 Console data is in progress. We will provide the next update by 00:00 PT.\r\n\r\n08/05 10:00 PT - Console Data Delay - Backfilling jobs for 8/3 are finished. Console data for 8/3 is up-to-date now. We are working on updating 8/4 Console data. We will provide the next update by 16:00 PT.\r\n\r\n08/04 23:45 PT - Console Data Delay - The team has made good progress on backfilling the aggregations for 8/3. We expect the backfilling jobs to finish within the next few hours. We will provide the next update by 08/05 10:00 PT.\r\n\r\n08/04 21:00 PT - Console Data Delay - The team is still working on backfilling the aggregations for 8/3. We will provide the next update by 00:00 PT.\r\n\r\n08/04 17:43 PT - We are actively monitoring the progress and backfilling the aggregations. We will provide the next update by 21:00 PT.\r\n\r\n08/04 11:00 PT - Degraded data processing starting around 8/3/2022 11:00am PT. Pulse data will be showing for 8/3/2022 but will be incomplete due to not all events being processed. Diagnostics charts and metric charts will show sporadic and incomplete data as well. No data has been lost and will be processed, we are actively backfilling these aggregations now.\r\n\r\n#### 6/28/2022 9:30hrs PT\r\n\r\n06/22 09:30 PT - We have mitigated the issue\r\n\r\n06/22 09:15 PT - Rolling back a server change impacting the initialize endpoint which could serve different initialize response payloads for the same input with no changes to the underlying gates/experiments\r\n\r\n06/22 03:23 PT - Slack reports of users switching between experiment groups\r\n\r\n#### 6/22/2022 10:00hrs PT\r\n\r\n06/22 13:40 PT - Data availability in Events Explorer has been restored.\r\n\r\n06/22 10:00 PT - We are actively investigating an ongoing issue affecting data availability in Events Explorer.\r\n\r\n\r\n#### 6/06/2022 12:10hrs PT\r\n\r\n\r\n6/06/2022 12:34hrs PT - the configuration was reverted and the issue was mitigated. Console is accessible again.\r\n\r\n6/06/2022 12:10hrs PT - a bad configuration was pushed to the statsig console app, taking it down across all regions.  API servers were completely unaffected.\r\n\r\n\r\n#### 5/26/2022 13:15hrs PT\r\n\r\n\r\n6/1/2022 14:00hrs PT - most lost events were recovered from logs.\r\n\r\n5/31/2022 10:31hrs PT - issue was mitigated by a server fix.\r\n\r\n5/31/2022 9:22hrs PT - the issue was first reported by a user.\r\n\r\n5/26/2022 13:15hrs PT - a server change was pushed, which caused certain events logged by the Ruby and Go SDKs to be dropped due to an error on the backend. The error logs were logged at \"info\" level, leading to us not discovering the issue at the time.\r\n\r\n#### 4/12/2022 22:00hrs PT\r\n\r\nFixed\r\n\r\nWe are unable to allocate VMs to run Databricks jobs. This will cause all analytics to be delayed. We have identified the root-cause to be a bug on Azure Billing side and have engaged with the Azure support team to resolve this as soon as possible.\r\n\r\n4/13/2022 15:30hrs PT Still working with Azure on resolution. We are running a pared down data pipeline to unblock everyone, while in parallel we figure out a full solution.\r\n\r\n20:00hrs PT The Azure issues have been resolved. Analytics are being processed and are catching back up. Incident impact was limited to processing delays; no data was lost.\r\n\r\n4/14/2022 15:00hrs All services back to normal. \r\n\r\n---\r\n\r\n#### 4/07/2022 10:00hrs PT\r\n\r\nFixed\r\n\r\nAn outage on Azure's Events Hub has impacted real time event streaming in Statsig, and outgoing integrations which replay events.\r\n\r\nWe're actively monitoring the issue and attempting to replay missing integration data.\r\n\r\n12:00 All services back to normal\r\n\r\n---\r\n\r\n#### 2/28/2022 12:46hrs PT\r\n\r\nMitigated/Fixed\r\n\r\n12:25 Offending change pushed to EU region;\r\n\r\n12:40 Offending change pushed to SEA region;\r\n\r\n12:46 Reports of iOS clients using anything below v1.8.0 of the iOS SDK were crashing\r\n\r\n13:05 Issue identified; started to revert recent changes;\r\n\r\n13:36 Changes reverted and issue mitigated. All services functioning normally.\r\n\r\n**Root Cause:**\r\n\r\nThe iOS SDK saves the response from Statsig's `initialize` endpoint to `UserDefaults` to be used to serve feature gate and experiment values when the user or Statsig is offline. Prior to v1.8.0, this was done via the [`UserDefaults.standard.setValue`](https://github.com/statsig-io/ios-sdk/blob/v1.7.3/Sources/Statsig/InternalStore.swift#L77) API. The problem with using this API is that it will crash if the value is a dictionary with any `nil` value in it.\r\n\r\nWe discovered the issue a couple weeks ago and fixed it in v1.8.1+ [here](https://github.com/statsig-io/ios-sdk/blob/v1.8.1/Sources/Statsig/InternalStore.swift#L95) by JSON serializing the payload first before saving to `UserDefaults`.\r\n\r\nStarting in v1.8.0, the SDK started only extracting fields explicitly and saving only those select fields into `UserDefaults`.\r\n\r\nCombined, these changes mean that adding new fields to the endpoint response, or returning nil for any existing field in the response, will not cause the SDK to crash in the future.  \r\n\r\nToday we deployed a change that introduced some new fields to the `initialize` endpoint's response, which are not used by the current versions of the SDK. However, one field has `nil` in the value, which resulted in crashes for versions below v1.8.0.\r\n\r\nMitigation:\r\n\r\nWe rolled back the changes to the `initialize` endpoint as soon as we discovered the issue, and since then crashes have stopped.\r\n\r\nPrevention:\r\n\r\n- v1.8.1+ of the iOS SDK contains the proper protections for similar potential issues\r\n- We have verified that the Android SDK already had these protections in place.  As always, we recommend staying up to date with the most recent version of Statsig SDKs (4.3.0 for Android)\r\n- we are updating the `initialize` endpoint to not return any `nil` before we deploy the change again\r\n- we are working on adding tests to ensure `nil` will not be included in the endpoint's response.\r\n\r\n---\r\n\r\n#### 2/24/2022 11:10hrs PT\r\n\r\nMitigated/Fixed\r\n\r\n11:10 Elevated rates of console and API errors related to a caching change.\r\n\r\n11:52 Resolved.  All services functioning normally.\r\n\r\n---\r\n\r\n#### 2/16/2022, 12:20hrs PT\r\n\r\nMitigated/Fixed\r\n\r\n13:01\r\nWe are aware of slow response times on our API pods.  We believe we have identified the underlying issue and are actively working on mitigating it.\r\n\r\n13:56\r\nWe have mitigated the issue and are spinning back the clusters back up.  Services should start to come back online soon.\r\n\r\n14:25\r\nAll services are back up and performing as expected.  \r\n\r\n15:26\r\nWe are working on identifying all affected customers and will be informing them in the next hour or so.\r\n\r\n---\r\n\r\n#### 2/5/2022, 18:00hrs PT\r\n\r\nMitigated/Fixed\r\n\r\nWe received reports of www and console not loading on Safari. Chrome worked fine. A refresh assisted by typeahead usually fixed the issue.\r\n\r\nUpon investigation we identified this is because of http -> https redirect that was broken when we moved from Kubernetes Ingress to Istio Gateway. The issue was resolved at 9AM on 2/6/2022."}